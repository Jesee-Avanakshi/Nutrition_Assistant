# -*- coding: utf-8 -*-
"""Project file.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1dD19YaKlb9mnE3jbFNX4JYK63Z2RaT4-
"""

!pip install pymupdf langchain langchain-community sentence-transformers chromadb bs4

import langchain
langchain.__version__

pip install langchain-google-genai

f = open("/content/gemini_key2.txt", "r")
api_key = f.read()

"""# **STEP 1: Imports**"""

import os
import requests
import urllib3
from bs4 import BeautifulSoup
import re

from langchain_core.documents import Document
from langchain_community.document_loaders import PyMuPDFLoader
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

"""# **STEP 2: Load all PDFS**"""

pdf_folder = "data/pdfs/"
pdf_files = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.endswith(".pdf")]

pdf_docs = []

for file in pdf_files:
    loader = PyMuPDFLoader(file)
    docs = loader.load()      # each page becomes a Document
    for d in docs:
        d.metadata["source"] = os.path.basename(file)
    pdf_docs.extend(docs)

print("PDF pages loaded:", len(pdf_docs))

"""# **STEP 3: SCRAPE WEB PAGES**"""

urls = [
    "https://www.who.int/news-room/fact-sheets/detail/healthy-diet",
    "https://www.who.int/news-room/fact-sheets/detail/food-safety",
    "https://www.diabetes.ca/nutrition-fitness/healthy-eating/healthy-eating-tips",
    "https://www.diabetes.ca/nutrition-fitness/healthy-eating/planning-healthy-meals",
    "https://www.diabetes.ca/nutrition-fitness/healthy-eating/carb-counting",
    "https://www.diabetes.ca/nutrition-fitness/healthy-eating/sugars-and-sweeteners",
    "https://nutritionsource.hsph.harvard.edu/2023/07/17/who-updated-guidelines-healthy-diets-total-fat/",
    "https://www.who.int/news-room/fact-sheets/detail/obesity-and-overweight"
]

headers = {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Accept-Language": "en-US,en;q=0.9",
}

#defining method to scrape the data from web
def scrape_and_clean(url):

    print(f"üîç Scraping: {url}")

    response = requests.get(url, headers=headers, timeout=20, verify=False)
    html = response.text
    soup = BeautifulSoup(html, "html.parser")

    # Remove noise
    for tag in soup(["script", "style", "header", "footer", "nav", "button", "form", "svg", "img"]):
        tag.decompose()

    # Try best main content
    main = soup.find("main") or soup.find("article") or soup.find("body") or soup

    text = main.get_text(separator="\n", strip=True)

    # Remove timestamps (video transcripts)
    text = re.sub(r"\b\d{1,2}:\d{2}\b", "", text)

    # Remove blank lines
    text = "\n".join([line for line in text.split("\n") if line.strip()])

    return text

web_docs = []

for url in urls:
    try:
        cleaned_text = scrape_and_clean(url)

        filename = url.split("/")[-1] or "index"
        filename = filename.replace("-", "_")

        web_docs.append(
            Document(
                page_content=cleaned_text,
                metadata={"source": filename, "type": "web"}
            )
        )

        print(f"Added web document: {filename}")

    except Exception as e:
        print(f"Error scraping {url}: {e}")

for doc in web_docs:
    filename = doc.metadata["source"] + ".txt"
    filepath = os.path.join("data/web", filename)

    with open(filepath, "w", encoding="utf-8") as f:
        f.write(doc.page_content)

    print(f"üìÑ Saved: {filepath}")

print("Web documents loaded:", len(web_docs))

"""# **Preprocessing**"""

# Combine everything into one list
all_docs = pdf_docs + web_docs
print("Total combined docs:", len(all_docs))

def clean_text(text):
    text = text.replace("\xa0", " ")
    text = text.replace("\n\n", "\n")
    text = text.strip()
    return text

for d in all_docs:
    d.page_content = clean_text(d.page_content)

#chunking
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,
    chunk_overlap=150,
    separators=["\n\n", "\n", ".", ","]
)

chunks = splitter.split_documents(all_docs)

print("Number of chunks:", len(chunks))

#Embeddings

from langchain_community.embeddings import HuggingFaceEmbeddings

embed_model = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")

embedding = embed_model.embed_query("What is a healthy diet?")
len(embedding)

from langchain_community.vectorstores import Chroma

vectordb = Chroma.from_documents(
    documents=chunks,
    embedding=embed_model,
    persist_directory="nutrition_db"
)

vectordb.persist()
print("Vector DB created and stored!")

retriever = vectordb.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 10}
)

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnableParallel, RunnablePassthrough
import os

os.environ["GOOGLE_API_KEY"] = api_key

llm = ChatGoogleGenerativeAI(
    model="gemini-2.5-flash",
    temperature=0.5
)

from langchain_core.prompts import PromptTemplate

prompt_template = """
You are a Nutrition Assistant trained on verified guidelines from:
- WHO (World Health Organization)
- USDA Dietary Guidelines for Americans
- Harvard School of Public Health
- Diabetes Canada

You MUST use the information provided in the context.
However, you are allowed to combine and summarize overlapping dietary principles
(even if the text does not explicitly mention WHO or USDA).

Do NOT invent facts.
Do NOT provide disease-specific diet plans.

If the context truly contains no relevant nutrition guidance, say:
"I don't have verified guideline information about this topic in the provided documents."

--------------------
CONTEXT:
{context}
--------------------

QUESTION:
{question}

Provide a concise answer based on the verified guidelines:
"""

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template=prompt_template
)

def expand_query(q):
    extra = " nutrition healthy diet guidelines fruits vegetables sugar salt fat recommendations"
    return q + extra

from langchain_core.runnables import RunnableLambda
from langchain_core.output_parsers import StrOutputParser
def format_docs(docs):
    return "\n\n".join([d.page_content for d in docs])

rag_chain = (
    {
        "context": RunnableLambda(expand_query) | retriever | RunnableLambda(format_docs),
        "question": RunnablePassthrough()
    }
    | prompt
    | llm
)

response = rag_chain.invoke("What does WHO recommend for a healthy diet?")
print(response)

# 1. Get retrieved docs
docs = retriever.invoke("What does WHO recommend for a healthy diet?")
print(docs)

# 2. Convert docs to text using format_docs
combined = format_docs(docs)
print("\n--- COMBINED TEXT (first 500 chars) ---\n")
print(combined)

# 3. Build prompt input manually
prompt_input = prompt.format(
    question="can you recommend for a healthy diet?",
    context=combined
)

print("\n--- FINAL PROMPT SENT TO LLM (first 500 chars) ---\n")
print(prompt_input)

raw_answer = llm.invoke(prompt_input)
print("\n--- LLM RAW ANSWER ---\n")
print(raw_answer.content)

"""# **Metrics**"""

eval_questions = [
    "What does WHO recommend for salt intake?",
    "How much free sugar should be consumed daily?",
    "How much fruit and vegetables should people eat?",
    "What does USDA say about saturated fat intake?",
]

gold_keywords = [
    ["5 g", "salt", "sodium"],
    ["<10%", "sugar", "5%"],
    ["400 g", "fruits", "vegetables"],
    ["10%", "saturated fat"],
]

from sklearn.metrics import precision_score, recall_score

def evaluate_retrieval():
    results = []

    for question, keywords in zip(eval_questions, gold_keywords):
        docs = retriever.invoke(question)
        text = " ".join([d.page_content for d in docs])

        found = any(kw.lower() in text.lower() for kw in keywords)
        results.append(found)

    recall_at_k = sum(results) / len(results)
    print("Recall@K:", recall_at_k)

evaluate_retrieval()

def evaluate_answer_quality():
    for q, keywords in zip(eval_questions, gold_keywords):
        answer = rag_chain.invoke(q).content.lower()

        relevance = any(kw.lower() in answer for kw in keywords)
        faithfulness = "i don't know" not in answer

        print("\nQ:", q)
        print("Relevance:", relevance)
        print("Faithfulness:", faithfulness)
        print("Answer:", answer[:200], "...")

evaluate_answer_quality()

import time

start = time.time()
_ = rag_chain.invoke("What is a healthy diet?")
end = time.time()

print("Response time:", round(end - start, 2), "seconds")

response = rag_chain.invoke("What does WHO recommend for fats?")
print(response.meta)

!zip -r nutrition_db.zip nutrition_db

from google.colab import files
files.download("nutrition_db.zip")

!pip freeze > requirements_colab.txt

from google.colab import files
files.download("requirements_colab.txt")

import pkg_resources

required_packages = [
    "streamlit",
    "langchain",
    "langchain-core",
    "langchain-community",
    "langchain-google-genai",
    "chromadb",
    "pymupdf",
    "sentence-transformers",
    "beautifulsoup4",
    "tiktoken",
    "requests",
]

installed = {pkg.key: pkg.version for pkg in pkg_resources.working_set}

with open("requirements_clean.txt", "w") as f:
    for pkg in required_packages:
        key = pkg.lower()
        if key in installed:
            f.write(f"{pkg}=={installed[key]}\n")

print("Clean requirements file created.")

from google.colab import files
files.download("requirements_clean.txt")